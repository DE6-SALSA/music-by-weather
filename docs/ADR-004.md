# ADR-004: [CSV vs. Parquet S3 to Redshift 적재 성능 비교 실험]

**📅 날짜**: 2025-07-23

**의사결정 배경**

S3에 저장된 데이터 파일을 Redshift에 COPY할 때, **CSV와 Parquet 포맷의 성능 차이** 를 정량적으로 측정하고자 함. 데이터 양이 많아질수록 적재 속도 및 효율 차이가 발생할 수 있으며, 이를 고려한 적재 포맷 결정을 위한 실험 필요.

**💡 고려된 옵션**

- CSV 포맷
- Parquet 포맷 (압축 방식: Snappy)

**실행방법**

Dummy Data 생성

- `generate_dummy_csv_parquet(size_mb: int, **kwargs)` 함수 작성
- 동일한 구조의 DataFrame을 기반으로 CSV 및 Parquet 파일 각각 생성
- schema: `artist, title, play_cnt, listener_cnt, tag1 ~ tag5, load_time`
- `play_cnt`, `listener_cnt`는 int32로 다운캐스팅하여 메모리 최적화

S3 업로드 & COPY

- S3에 각각 `benchmark/csv/`, `benchmark/parquet/` 폴더로 저장
- Redshift COPY 시각 기준 성능 측정: `redshift_copy_time_secs` 컬럼

메타데이터 기록

- `analytics.format_comparison_benchmark` 테이블에 실험 결과 기록
- 저장 항목: run_id, format, record_count, file_size_mb, s3_key, redshift_copy_time_secs, created_at

**✅ 결정**

Parquet 채택

**🏷️ 근거**

- 실험 결과, 중소량 데이터(예: 7천~35만건 수준)에서는 **CSV와 Parquet 모두 COPY 속도 1초 내외**로 처리됨
- CSV는 사람이 읽을 수 있는 **텍스트 포맷**이므로, 테스트 및 에러 디버깅에 유리
- Parquet은 `pyarrow` 등 별도 패키지, 정밀한 스키마 컨트롤 필요
- **Redshift COPY에 안정적으로 적합**하며, 중소 규모에서는 **성능상 큰 손해 없이도 운영 가능** 하기 때문에, **프로토타이핑, ETL 안정성 우선시, 협업 고려** 상황에서 **충분히 선택할 수 있는 합리적 옵션**
